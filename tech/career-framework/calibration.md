# Calibration (Consistency Checks)

With a large organization, it is important that we are consistent in the evaluation across various families, teams and managers. To achieve consistency across the board, there are several routines that take place during the performance review cycle:
- Calibration sessions trainings for Managers
- Pre-review Calibration
- Data-based Sanity Check
- Lessons Learned
	
  
## Training - Calibration sessions trainings for Managers

Having people aligned on expectations from each role up front is the most efficient way. We are going to have a sessions with managers before each performance review cycle to make sure everyone has the same understanding of:
- Impact expectations from each Level (Beginner, Intermediate, Advanced, Expert, Leading Expert) - especially for `Expert` and `Leading Expert` level there is sometimes confusion what is the expected impact of relevant competencies 
- Are we looking for these expectations as grow aspect or performance (role responsibility) aspect? Competencies once achieved are not guaranteed to be retained for ever
- What should be the manager's action if people are under-performing or over-performing.
- Make sure everyone is aware of additional resources - Examples of Competency Interpretations. Opportunity to discuss questions within larger group to get more opinions.
		
    
## Pre-review Calibration

This step helps us to catch possible inconsistencies early in the cycle, before the final performance ratings would be communicated to employees. The process is following:
- Employees fill in their self-rating in the competency spreadsheet and submit it to their manager AHEAD of the performance review meeting. Employees are encouraged to ask their colleagues for feedback before submitting the self-review.
- Managers prepare draft of the performance reviews for the employees based on their self-evaluation, feedback collected and performance observed over the last 6 months.
- Managers submit the proposed score to their superiors (eg. Engineering Managers submit proposed score of their ICs to Engineering Directors). The draft score is added to the global spreadsheet.
- Pre-review calibration sessions are organized within various groups:
  - For each family, its Engineering Director and all Engineering Managers meet and go through:
    - For all promotions to senior roles and higher (eg. to Senior Engineer and higher for IC track), discuss justification/achievements
    - Review list of people not progressing at all, people progressing significantly faster than others - discuss 
  - For small families, it makes sense to do a combined session with members of 2 families together. The similar calibration sessions should also happen on higher levels:
    - VP & Engineering Directors reviewing draft ratings of Engineering Managers
    - CTO & VPs reviewing draft ratings of Engineering Directors

- Manager might be asked/encouraged to change proposed rating for employee if it turns out that his/her original evaluation of employee wasn't consistent with other managers.

Please note that we are still talking only about draft - proposed - rating. While often this will accurately represent the fair employee's assessment, there can be situations when some new important information is comes up on the actual performance review meeting with employee later and it can lead to rating adjustment in either direction. That is fine but such occurrences should be relatively rare.


## Data-based Sanity Check

There is no good way how to do detailed calibration check across large organization or large number of teams. Therefore   we are doing just a sanity check on statistical data collected:

- During this exercise, we are looking at the proposed ratings already collected (see Pre-review Calibration chapter).
- Latest proposals for performance review score is in the global tech spreadsheet. This sheet contains historical ratings too so this is a data source that can be used to understand the 'delta' (change of employees' progress since previous performance review)
- Identify calibration groups that you want to look into - calibration group is a combination of a grade (e.g. IC3 or IC4) and a track (e.g. Engineer). It doesn't make sense to compare career growth of employees on different tracks and also people of different seniority (e.g. IC2 against IC4). So one example of calibration group could be `Engineers who were at IC3 grade before this review cycle`
- See distribution of progress delta of each employee within the calibration group. Check for anomalies. Then review if the distribution is similar across families and teams with large enough statistical samples. The goal of this exercise is not to remove any anomalies, but to understand why it is that way and if it is fair. It is just an indicator that starts subsequent discussion.
- Does the progress delta distribution look similar across all families? Should we check if managers in one family are too harsh or too nice?
- Do people at the beginning of their career grow faster than experienced employees with many years of experience? It is normal that people grow very quickly in their first job, while pace of progress growth typically slows down the higher the employee already is on their career ladder.


## Lessons Learned

After each performance review cycle, there should be a Lessons Learned session for managers, where they could discuss, share and learn from each other:
- What worked for them during review. What surprised them.
- Difficult conversations they had with their reports and how they handled it
- Propose updates to examples of competency interpretations,
- Propose changes to the performance review process

